{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed_value = 42\n",
    "\n",
    "# TODO: assert input right shape for nn\n",
    "\n",
    "class network:\n",
    "    def __init__(self, layers: np.array, seed_value: float = None):\n",
    "        \n",
    "        '''\n",
    "        Implementing a fully connected neural network.\n",
    "        The network consists of two np.arrays, namely weights and biases.\n",
    "        weights[l] = weights matrix of the l+1-th layer. --> weights[l][j][i] = w_{ji}_{l+1}\n",
    "        biases[l] = biases of the l+1-th layer. --> biases[l][i] = b^{l+1}_{i}\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        np.random.seed(seed_value)\n",
    "        self.weights = [np.random.rand(layers[i+1], layers[i]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.random.rand(layers[i],1) for i in range(1, len(layers))]\n",
    "\n",
    "    def forward(self, z : np.array)->np.array:\n",
    "        z = [z] # need to do this with a list, since not all sublist are of the same size\n",
    "        a = z[:] # create copy of z\n",
    "\n",
    "        z.append(np.dot(self.weights[0], a[0]) + self.biases[0][0])        \n",
    "        a.append(ReLU(z[-1]))\n",
    "\n",
    "\n",
    "        for i in range(1,len(self.layers)-1):\n",
    "            z.append(np.dot(self.weights[i], a[-1]) + self.biases[i][0])\n",
    "            a.append(ReLU(z[-1]))\n",
    "        return z , a # returns two lists\n",
    "    \n",
    "    def SGD(self, train_data: np.array, eta: float, epochs: int, batch_size: int, loss_fn, dloss_fn, report: bool = False, validation_data: np.array = None, seed_value: float = None):\n",
    "        '''if report is true, then we need validation data'''\n",
    "        data = train_data\n",
    "        np.random.seed(seed_value)\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(data) # in place\n",
    "            for i in range(0,len(data), batch_size):\n",
    "                self.batch_update(data[i:i+batch_size], eta, loss_fn=loss_fn, dloss_fn=dloss_fn)\n",
    "\n",
    "            if report:\n",
    "                # might be more interesting to calculate loss and accuracy after each batch update, not after each epoch\n",
    "                loss, accuracy = 0, 0\n",
    "\n",
    "                for (X,y) in validation_data:\n",
    "                    y_pred = self.forward(X)[-1]\n",
    "                    loss += loss_fn(y_pred=y_pred, y_true= y)\n",
    "                    # this should be done better\n",
    "                    temp = np.zeros_like(y_pred)\n",
    "                    temp[np.argmax(y_pred)] = 1\n",
    "                    accuracy += np.array_equal(temp, y)\n",
    "\n",
    "                loss /= len(validation_data)\n",
    "                accuracy /= len(validation_data)\n",
    "                print(f\"epoch: {epoch} | loss: {loss} | accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "    def batch_update(self, train_data: np.array, eta: float, loss_fn, dloss_fn):\n",
    "        partial_weights = np.zeros_like(self.weights)\n",
    "        partial_biases = np.zeros_like(self.biases)\n",
    "\n",
    "        for (X,y) in train_data:\n",
    "            weights_update, biases_update  = self.backpropagation(X, y, dloss_fn)\n",
    "            partial_weights += weights_update\n",
    "            partial_biases += biases_update\n",
    "        \n",
    "        partial_weights = [(-eta*sublist)/len(train_data) for sublist in partial_weights] # normalize and multiply by learning rate\n",
    "        partial_biases = [(-eta*sublist)/len(train_data) for sublist in partial_biases]\n",
    "\n",
    "\n",
    "        self.weights = [np.add(x,y) for (x,y) in zip(self.weights, partial_weights)]\n",
    "        self.biases  = [np.add(x,y) for (x,y) in zip(self.biases, partial_biases)]\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagation(self, X: np.array, y: np.array, dloss_fn):\n",
    "        z, a = self.forward(X)\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "\n",
    "        \n",
    "\n",
    "        delta = z[1:] # so that delta is initialized with the correct size\n",
    "        delta.reverse() # error in the last layer is in the first entry of delta, etc.\n",
    "        delta[0] = dloss_fn(y_true=y, y_pred=a[-1]) * dReLU(z[-1])\n",
    "        print(f\"Print delta: {delta}\")\n",
    "        print(f\"Print weights\")\n",
    "\n",
    "        for l in range(1, len(delta)):\n",
    "            delta[l] = (np.dot(np.transpose(self.weights[-l]), delta[l-1])) * dReLU(z[-(l+1)])\n",
    "\n",
    "        delta.reverse()\n",
    "\n",
    "        return self.weights, delta # partial_weights is the same as delta according to backpropagation formula 3)\n",
    "\n",
    "def ReLU(input: np.array):\n",
    "        return np.maximum(0,input)\n",
    "\n",
    "def dReLU(input: np.array):\n",
    "        return np.where(input>0, 1, 0)\n",
    "\n",
    "# when using the network with ReLU one can't easily use cross entropy loss\n",
    "def mean_square_error(y_true: np.array, y_pred: np.array):\n",
    "    '''square mean error for one training input'''\n",
    "    return np.mean(np.square(y_pred-y_true))\n",
    "     \n",
    "\n",
    "def dmean_square_error(y_true: np.array, y_pred: np.array):\n",
    "    return 2/len(y_true) * (y_pred-y_true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1,  12,   3],\n",
       "        [124,   2,   1]],\n",
       "\n",
       "       [[  1,   2,   4],\n",
       "        [  5,   3,   6]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [[[1, 12, 3], [124, 2, 1]],[[1, 2, 4],[5, 3, 6]]]\n",
    "np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n",
      "a[5388377.531935   5167253.28350179]\n",
      "b[2320.28811911 2271.65981257]\n"
     ]
    }
   ],
   "source": [
    "layer = [2,3,2,7,3,346,2]\n",
    "net = network(layer, 43)\n",
    "a,b = net.backpropagation(np.array([1,2]), np.array([2,3]), dmean_square_error)\n",
    "print(len(a), len(b))\n",
    "print(f\"a{a[-1]}\")\n",
    "print(f\"b{b[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
